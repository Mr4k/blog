<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>peterstefek.me - probability</title>
        <link rel="stylesheet" href="/theme/css/main.css" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="peterstefek.me Atom Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">peterstefek.me </a></h1>
                <nav><ul>
                    <li><a href="/category/algorithms.html">algorithms</a></li>
                    <li><a href="/category/graphics.html">graphics</a></li>
                    <li><a href="/category/probability.html">probability</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/optimal-stopping.html">Optimal Stopping Policies</a></h1>
<footer class="post-info">
        <abbr class="published" title="2018-10-06T00:00:00-07:00">
                Published: Sat 06 October 2018
        </abbr>
		<br />
        <abbr class="modified" title="2019-10-15T00:00:00-07:00">
                Updated: Tue 15 October 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/peter-stefek.html">Peter Stefek</a>
        </address>
<p>In <a href="/category/probability.html">probability</a>.</p>
<p>tags: <a href="/tag/probability.html">probability</a> <a href="/tag/algorithms.html">algorithms</a> </p>
</footer><!-- /.post-info --><p><strong>The Recruiter Problem</strong><br>
Pretend for a minute that you are a recruiter who's looking for people to fill a position. Maybe you're reading this post because I have applied for a position at your company and you're vetting me. Anyways the point is I'm one candidate among many. Even if you interview me and like me how do you know there's not someone better out there to fill the position? <br><br>
More formally, you are going to interview <span class="math">\(N\)</span> candidates for a job. The order in which they come is random, with all orderings equally likely. After each interview with a candidate you will come up with a quality score (a real number between 0 and 1) for them and then you have to choose whether to hire them on the spot or reject them and hope a better fit comes along. Your goal is to hire the best of the best (the candidate who has the highest score among all N candidates) but you have no idea what the applicant pool looks like (no information about the distribution from which these candidates are drawn). What is the probability that you succeed in your task given that you use the optimal hiring strategy? <br><br>
This problem is know sometimes as the best choice problem or the secretary problem. We will call it the Recruiter Problem. It is a very famous and well studied problem because of its very surprising solution: <br><br>
<div style="text-align:center;"> <span class="math">\(\lim_{N \to \infty}Pr(\mbox{best is selected out of N candidates}) \to \frac{1}{e}\)</span> </div><br>
This is a really cool result for several reasons. First of all it's not even clear that there should be any kind of one fits all strategy. Secondly, <span class="math">\(\frac{1}{e} = 0.367879...\)</span> which seems absurdly high. If have a million candidates, you have around a one third chance of choosing <strong>the</strong> very best one. Here is a graph showing the probability of success using this strategy for different numbers of candidates,<br>
<p align="center">
    <img src="/images/optimal-stopping/cutoff.png" width="100%" > 
</p><br> The strategy is also fairly simple and seemingly bizarre.<br><br>
<strong>Strategy</strong><br> For every N there is an <span class="math">\(r_N^*\)</span> such that the following strategy is optimal. Reject the first <span class="math">\(r_N^* - 1\)</span> percent of the candidates then choose the next candidate who is better all of then them. <br><br>
This type of strategy is called a cutoff rule. The final surprising fact is that <br>
<div style="text-align:center;"> </p>
<div class="math">$$\lim_{N \to \infty} r_N^* \to \frac{1}{e}$$</div>
<p> </div>
This means that you see less than half of the many candidates while making your decision. <br><br>
<strong>Analysis</strong><br>
Analyzing the cutoff rule approach to this problem is fairly straightforward. Consider the cutoff rule with a variable <span class="math">\(r\)</span> which defines the cutoff point. The probability of finding the max can be though as follows, <br>
<div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=1}^N P(\mbox{applicant i is selected} \cap \mbox{applicant i is the best})$$</div>
<p> </div>
<div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=1}^N P(\mbox{applicant i is selected} \vert \mbox{applicant i is the best}) \times P(\mbox{applicant i is the best})$$</div>
<p> </div>
The probability of any applicant being the best if <span class="math">\(\frac{1}{n}\)</span> because our candidates are equally likely to appear in any order. So
<div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=1}^N(\mbox{applicant i is selected} \vert \mbox{applicant i is the best}) \times \frac{1}{n}$$</div>
<p> </div> <br>
The probability of success is clearly zero if when the best candidate is in the first r candidates since we reject them all. So we can simplify the above expression as follows: <br>
<div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=r}^N(\mbox{applicant i is selected} \vert \mbox{applicant i is the best}) \times \frac{1}{n}$$</div>
<p> </div> <br>
Finally we have one last key insight. We can observe that for <span class="math">\(i \geq r\)</span>, <span class="math">\(P(\mbox{applicant i is selected} \vert \mbox{applicant i is the best})\)</span> is the same as saying that the best of the first <span class="math">\(i - 1\)</span> candidates is in first r - 1 candidates. 
<div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=r}^N \frac{r-1}{i-1} \times \frac{1}{n} = \frac{r-1}{n}\sum_{i=r}^N \frac{1}{i-1}$$</div>
<p> </div> <br>
We can observe as <span class="math">\(n \to \infty\)</span> that our probability becomes:
<div style="text-align:center;"></p>
<div class="math">$$P(x)=x\int_x^1 \frac{1}{t}dt$$</div>
<p></div> <br>
Maxmimizing this function in terms of x gives us <span class="math">\(x^*=\frac{1}{e}\)</span> and <span class="math">\(P(x) = \frac{1}{e}\)</span>.</p>
<p><br><br>
<strong>A Savvier Recruiter</strong><br>
It turns out that in the case where we know nothing about the distribution, the cutoff rule is optimal. But what if we do know something? This time let's pretend we know our distribution is uniform on <span class="math">\([0,1]\)</span>. We will call this problem the Savvy Recruiter Problem. Now let's try to come up with an optimal strategy for the Savvy Recruiter Problem. After each interview we have to accept or reject the candidate we just interviewed (candidate i). To gain some insight we can take a look at the following statement, <br>
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{best candidate was in past}) + P(\mbox{candidate i is best}) + P(\mbox{best candidate is in future}) = 1$$</div>
<p></div>
Now we don't have control over the past but we do have the choice between hiring i and rejecting i. Therefore we choose the option which has the highest probability of giving us the best candidate. Let's make a few observations about these probabilities. <br><br>
If candidate i's score is less than any of the previous candidates we know that:
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{candidate i is best}) = 0$$</div>
<p></div>
In this case we would always choose to move on because no matter what our chance of finding the best candidate in the future is we know it is greater than 0. <br><br>
Now what if candidate i is better than all the other candidates we've seen so far? Off the bat we know that, <br>
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{best candidate was in past}) = 0$$</div>
<p></div>
This means that,
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{candidate i is best}) + P(\mbox{best candidate is in future}) = 1$$</div>
<p></div>
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{best candidate is in future}) = 1 - P(\mbox{candidate i is best})$$</div>
<p></div><br>
Let <span class="math">\(q_i\)</span> denote candidate i's quality score. What we want now is the probability that we will not find a better candidate in our last <span class="math">\(N-i\)</span> interviews. Since our candidate scores are drawn from a uniform distribution on <span class="math">\([0,1]\)</span>,<br>
<div style="text-align:center;"></p>
<div class="math">$$ P(\mbox{candidate j's score } \leq q_i) = q_i, \forall j $$</div>
<p> </div>
<div style="text-align:center;"></p>
<div class="math">$$ P(\mbox{last N-i candidates are all } \leq q_i) = \prod_{j = i+1}^{N}P(\mbox{candidate j's score } \leq q_i) = q_i^{N-i} $$</div>
<p> </div>
<div style="text-align:center;"></p>
<div class="math">$$ P(\mbox{best candidate in future}) = 1 - P(\mbox{last N-i candidates are all } \leq q_i) = 1 - q_i^{N-i} $$</div>
<p> </div>
So when is it optimal to choose to accept candidate i? It is when,
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{candidate i is best}) \geq P(\mbox{best candidate is in future})$$</div>
<p></div>
Equivalently this when,
<div style="text-align:center;"></p>
<div class="math">$$P(\mbox{best candidate is in future}) \leq \frac{1}{2}$$</div>
<p></div>
<div style="text-align:center;"></p>
<div class="math">$$1 - q_i^{N-i} \leq \frac{1}{2}$$</div>
<p></div>
Using this information we can solve for the threshold <span class="math">\(q^*_i\)</span> such that if <span class="math">\(q_i \geq q^*_i\)</span> our best option is to choose candidate i.
<div style="text-align:center;"></p>
<div class="math">$$1 - {q^*_i}^{N-i} = \frac{1}{2}$$</div>
<p></div>
<div style="text-align:center;"></p>
<div class="math">$${q^*_i}^{N-i} = \frac{1}{2}$$</div>
<p></div>
<div style="text-align:center;"></p>
<div class="math">$$q^*_i = \frac{1}{2}^\frac{1}{N-i}$$</div>
<p></div><br></p>
<p>So how much better is a savvy recruiter than a regular recruiter? Below is a graph comparing the probabilities that each type of recruiter will choose the best candidate. <br>
<p align="center">
    <img src="/images/optimal-stopping/cutoff-vs-knowing.png" width="100%"/> 
</p><br/>
<details>
    <summary>(details)</summary>
    The probability of success of the savvy recruiter strategy is a little more difficult to calculate then that of the basic recruiter strategy. However we will follow the same basic idea. First we break the probability into the sum of the probabilities of succeeding at each step (we can do this because the events are mutually exclusive),
    <div style="text-align:center;"> </p>
<div class="math">$$P(\mbox{success}) = \sum_{i=1}^N P(\mbox{we pick the max at step i})$$</div>
<p> </div> 
    However this is where it gets a little tricky. Remember that we pick a candidate if they are both larger than the threshold for that step <span class="math">\(q_i\)</span> and they are the max we have seen so far. Furthermore the sequence optimal thresholds <span class="math">\((q_i)\)</span> is decreasing. This means that there are situations where a candidate i can be larger than than the ith threshold <span class="math">\(q_i\)</span> but still be passed over because there was already a larger candidate that did not exceed a larger previous threshold. 
    <p align="center">
        <img src="/images/optimal-stopping/threshold-not-selected.png" width="40%" > 
    </p><br>
    In the above image the current candidate (red) will not be selected even though it is larger than its threshold. To help us out with this problem we will define the true threshold <span class="math">\(t_i\)</span>. The true threshold <span class="math">\(t_i\)</span> is the max of the threshold <span class="math">\(q_i\)</span> and the last <span class="math">\(i - 1\)</span> candidate values. If the candidate at step <span class="math">\(i\)</span> is above the true threshold then they we be picked. So at step i we can condition on the true threshold.
    <div style="text-align:center;"></p>
<div class="math">$$P(\mbox{success at step i}) = P(\mbox{success at step i} | t_i = q_i)P(t_i=q_i) + P(\mbox{success at step i } | q_i &lt; t_i \leq 1)P(q_i&lt;t_i\leq 1)$$</div>
<p></div>
    We can compute this larger term in two parts,
    <br><b>Unchanged Threshold</b>
    <div style="text-align:center;"></p>
<div class="math">$$P(\mbox{success at step i} | t_i = q_i)P(t_i=q_i)$$</div>
<p></div>
    We know that we are successful at step i if and only if candidate i is larger than <span class="math">\(t_i\)</span> (so we pick them) and candidate i is the max. We can write this as,
    <div style="text-align:center;"></p>
<div class="math">$$P(\mbox{success at step i | t_i = q_i}) = \int_{q_i}^1 F_{X+1}(x)...F_{X+n}(x)f_{X_i}(x)$$</div>
<p></div>
    Because all the candidates are drawn i.i.d from a uniform distribution we can further simplify,
    <div style="text-align:center;"></p>
<div class="math">$$P(\mbox{success at step i | t_i = q_i}) = \int_{q_i}^1 x^{n-i}dx$$</div>
<p></div>
    Next let's look at
    <div style="text-align:center;"></p>
<div class="math">$$P(t_i = q_i)$$</div>
<p></div>
    We know this happens only when no previous candidate was larger than <span class="math">\(q_i\)</span> so we can express this as,
    <div style="text-align:center;"></p>
<div class="math">$$P(t_i = q_i) = q_i^{i-1}$$</div>
<p></div>
    Combining these two terms we get
    <div style="text-align:center;"></p>
<div class="math">$$P(\mbox{success at step i} | t_i = q_i)P(t_i = q_i) = q_i^{i-1}\int_{q_i}^1 x^{n-i}dx$$</div>
<p></div>
    <b>Modified Threshold</b>
    <div> </p>
<div class="math">$$P(\mbox{success at step i } | q_i &lt; t_i \leq 1)P(q_i&lt;t_i\leq 1) $$</div>
<p></div>
    <div> </p>
<div class="math">$$P(\mbox{success at step i } | q_i &lt; t_i \leq 1)P(q_i&lt;t_i\leq 1) = \sum_{k=1}^{i}P(\mbox{success at step i } | q_{k}\leq t_i \leq q_{k-1})P(q_{k}\leq t_i \leq q_{k-1}) $$</div>
<p>(where <span class="math">\(q_0 := 1\)</span> for notational convience)</div>
    <div> </p>
<div class="math">$$P(\mbox{success at step i } | q_i &lt; t_i \leq 1)P(q_i&lt;t_i\leq 1) = \sum_{k=1}^{i}\int_{q_{k}}^{q_{k - 1}}P(\mbox{success at step i } |  q_{k}\leq t_i \leq q_{k-1})f_{t_i}(x)dx $$</div>
<p></div>
    <div> </p>
<div class="math">$$P(\mbox{success at step i } | q_i &lt; t_i \leq 1)P(q_i&lt;t_i\leq 1) = \sum_{k=1}^{i}\int_{q_{k}}^{q_{k - 1}}P(\mbox{success at step i } |  q_{k}\leq t_i \leq q_{k-1})f_{t_i}(x)dx $$</div>
<p></div>
    Now after combinining both the unchanged and modified thresholds in a few more brutal algebrabic simplications we end up with the following:
    </p>
<div class="math">$$P(\mbox{success at step i}) = \bigg(\frac{1}{n-i+1}\bigg)\bigg[t_i^{i-1}-t_i^n+\sum_{j=1}^{i - 1}j \bigg(\frac{x^{i-1}}{i-1}-\frac{x^n}{n-1}\bigg)\bigg\rvert_{t_{j+1}}^{t_j}\bigg]$$</div>
<div class="math">$$P(\mbox{success with n candidates}) = \sum_{i=1}^n\bigg(\frac{1}{n-i+1}\bigg)\bigg[t_i^{i-1}-t_i^n+\sum_{j=1}^{i - 1}j \bigg(\frac{x^{i-1}}{i-1}-\frac{x^n}{n-1}\bigg)\bigg\rvert_{t_{j+1}}^{t_j}\bigg]$$</div>
</details>

<p>So the savvy recruiter strategy is clearly better for a uniform distribution. This shouldn't be suprising because the savvy recruiter has more information than the basic recruiter. But just to be thorough we want to know if our results would be different if our underlying distribution was not uniform. It turns out it doesn't matter. We can actually transform any distribution into a uniform distribution by looking at percentiles then apply both strategies for a uniform to it. This works in our specific case because we care about finding the best candidate and the transformation into percentiles preserves the order of the candidate values. It's important to note that this type of transformation does not work if we care about maxmimizing other statistics such as expected value.  <br><br></p>
<p>So clearly we have a much better chance of picking the max if we know the underlying distribution from which our candidates are drawn. However in most situations the true distribution isn't usually known (some people would even say that the idea of a single true distribution is ridiculous but we will not address that here for simplicities sake). But what if we could use the information we see during our interviews to appoximate the true distribution? Then we could use the savvy recruiter strategy with the our best guess of the true distribution. Seems kind of cool right? Let's look at an example. <br><br></p>
<p align="center">
    <img src="/images/optimal-stopping/apartments.jpg" width="100%" > 
</p>

<p><br></p>
<p>Let's say that you have decided to get a house. However you live in a competitive market so housing tends to go fast. You need a house with in the next month so not a lot of new houses will come onto the market (let's say that you would be able to see a maximum of <span class="math">\(n\)</span> houses). Once you've seen a house, you basically have to decide whether or not to buy it on the spot. By the time you've seen another the first house will probably already be gone. You also want to find the best house for yourself. We will assume we can model the distribution of house qualities around the area with a gaussian distribution but you do not know its parameters (the mean or the variance). So how do you find the best house for yourself?
<br><br>
As we view houses we attempt to learn the true underlying distribution of house qualities. The simplest way to do this is to use a technique called Maximum Likelihood Estimation. Maximum Likelihood Estimation operates on the assumption that each draw from the distribution is independent. It finds the parameters of a distribution (in our case the mean and the variance) which maximize the likelihood of seeing our observed data points. <br><br>
For a Gaussian distribution the likelihood function is given observed points <span class="math">\(x_1,...x_n\)</span> is defined as follows:<br>
<div style="text-align:center;"></p>
<div class="math">$$ L(\mu, \sigma^2) = p(X_1=x_1,...X_n=x_n \vert \mu, \sigma^2) = p(X_1 = x_1 \vert \mu, \sigma^2)...p(X_n = x_n \vert \mu, \sigma^2) $$</div>
<p></div>
Notice that because each draw is independent the probability of seeing all of them is just the products of the probabilities of seeing each one individually! It turns out that the parameters (<span class="math">\(\mu, \sigma^2\)</span>) which maximize the likelihood of seeing n data points are given by, 
<div style="text-align:center;"></p>
<div class="math">$$\mu =  \frac{1}{n}\sum_{i=1}^n x_i = \bar x  $$</div>
<p></div>
<div style="text-align:center;"></p>
<div class="math">$$ \sigma^2 =  \frac{1}{n}\sum_{i=1}^n (x_i - \bar x)^2  $$</div>
<p></div>
<details>
    <summary>(details)</summary> Remember that we are looking for the parameters <span class="math">\(\mu, \sigma^2\)</span> which maximize our likelihood function. Now generally products are pretty gross to deal. One common trick when dealing with maximizing products is to maximize the sum of the logs instead. Because log is monotonically increasing function on its domain (and we know the maximum of likelihood function is greater than zero) finding the parameters which maximize the likelihood is equivalent to finding parameters which maximize the log likelihood. For a gaussian distribution our log likelihood function can be derived as follows: <br>
    <div style="text-align:center;"></p>
<div class="math">$$ L(\mu, \sigma^2) = p(X_1=x_1,...X_n=x_n \vert \mu, \sigma) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x_i - \mu)^2}{2\sigma^2}} $$</div>
<p></div>
    The taking the log of both sides gives us the log likelihood: <br>
    <div style="text-align:center;"></p>
<div class="math">$$ \mbox{ln}(L(\mu, \sigma^2)) = \sum_{i=1}^n \mbox{ln}(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{(x_i - \mu)^2}{2\sigma^2} $$</div>
<p></div>
    <div style="text-align:center;"></p>
<div class="math">$$ \mbox{ln}(L(\mu, \sigma^2)) = n\mbox{ln}(\frac{1}{\sqrt{2\pi\sigma^2}}) - \sum_{i=1}^n \frac{(x_i - \mu)^2}{2\sigma^2} $$</div>
<p></div>
    <div style="text-align:center;"></p>
<div class="math">$$ \mbox{ln}(L(\mu, \sigma^2)) = -\frac{n}{2}\mbox{ln}(\pi)-\frac{n}{2}\mbox{ln}(\sigma^2) - \sum_{i=1}^n \frac{(x_i - \mu)^2}{2\sigma^2} $$</div>
<p></div>
    Now we can take partial derivatives with respect to both <span class="math">\(\mu\)</span> and <span class="math">\(\sigma^2\)</span> (the mean and the variance). <br>
    <div style="text-align:center;"></p>
<div class="math">$$ \frac{\mbox{ln}(L(\mu, \sigma^2))}{\partial \mu} =  \sum_{i=1}^n \frac{x_i - \mu}{\sigma^2}  $$</div>
<p></div>
    <div style="text-align:center;"></p>
<div class="math">$$ \frac{\mbox{ln}(L(\mu, \sigma^2))}{\partial \sigma^2} =  -\frac{n}{\sigma^2} + \sum_{i=1}^n \frac{(x_i - \mu)^2}{\sigma^4}  $$</div>
<p></div>
    Setting the partial derivatives to zero we get the following system of equations. <br>
    <div style="text-align:center;"></p>
<div class="math">$$ \sum_{i=1}^n \frac{x_i - \mu}{\sigma^2} = 0 $$</div>
<p></div>
    <div style="text-align:center;"></p>
<div class="math">$$ -\frac{n}{\sigma^2} + \sum_{i=1}^n \frac{(x_i - \mu)^2}{\sigma^4} = 0 $$</div>
<p></div>
    From the first equation we obtain: <br>
    <div style="text-align:center;"></p>
<div class="math">$$ \mu =  \frac{1}{n}\sum_{i=1}^n x_i = \bar x  $$</div>
<p></div>
    Plugging that into the second equation we get: <br>
    <div style="text-align:center;"></p>
<div class="math">$$ \sigma^2 =  \frac{1}{n}\sum_{i=1}^n (x_i - \bar x)^2  $$</div>
<p></div>
</details></p>
<p>We can now use these MLE estimates to approach our stopping problem with the optimal strategy for a known distribution. For each candidate house i, we first come up with an estimate of our parameters using all of the houses we have seen so far. Then we use this estimated distribution to calculate the percentile of the candidate house. Finally we check to see if the current candidate is the max we have seen. If so the probability that our candidate house is the best is greater than <span class="math">\(\frac{1}{2}\)</span> we choose it (we can do this the same way as in the thresholding strategy for a known distribution). Here are several graphs demonstrating results of this approach:
<details style="display:inline;">
    <summary>(details)</summary>
    We run simulations 10000 times (for each n) then average together to get the final probability of success for each n. Each time we run we pick our "true" parameters of underlying normal distribution from two different uniform random distributions. <span class="math">\(\mu\)</span> ~ <span class="math">\(Unif(0, 1000)\)</span>, <span class="math">\(\sigma\)</span> ~ <span class="math">\(Unif(0, 15)\)</span>. 
</details></p>
<p align="center">
    <img src="/images/optimal-stopping/learning-250.png" width="100%" > 
</p>

<p><br></p>
<p>Zooming in we can see that the cutoff policy is on par with the learned policy until there are around 14 houses on the market.</p>
<p align="center">
    <img src="/images/optimal-stopping/learning-35.png" width="100%" > 
</p>

<p><br></p>
<p>Finally if we look at a much larger number of available houses we can see that the learning strategy gets much better over time.</p>
<p align="center">
    <img src="/images/optimal-stopping/learning-2000.png" width="100%" > 
</p>

<p><br></p>
<p><strong>Limitations</strong><br>
So this is cool little fact but what kinds of problems can we apply it too besides that contrived housing example? Could we use it to made the best stock market trades or decide when to bid on ebay? Unfortunately the answer is no. One large underlying assumption in our model is that the candidate qualities are each drawn independently from an underlying distribution. Many types of time series like the stock market or the price of ebay bids are highly correlated. For example if one tech stock goes down its likely others will have gone down with it. Under these conditions our model would mostly likely work very poorly. <br> <br>
<br><br><br></p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/alias-method.html" rel="bookmark"
                           title="Permalink to The Alias Method for Fast Weighted Random">The Alias Method for Fast Weighted Random</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-06-25T00:00:00-07:00">
                Published: Sun 25 June 2017
        </abbr>
		<br />
        <abbr class="modified" title="2019-10-15T00:00:00-07:00">
                Updated: Tue 15 October 2019
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/peter-stefek.html">Peter Stefek</a>
        </address>
<p>In <a href="/category/algorithms.html">algorithms</a>.</p>
<p>tags: <a href="/tag/probability.html">probability</a> <a href="/tag/algorithms.html">algorithms</a> </p>
</footer><!-- /.post-info -->                <p>Constant time draws from a discrete distribution.</p>
                <a class="readmore" href="/alias-method.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>links</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                            <li><a href="#">You can modify those links in your config file</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>
                            <li><a href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="#">You can add links in your config file</a></li>
                            <li><a href="#">Another social link</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

</body>
</html>